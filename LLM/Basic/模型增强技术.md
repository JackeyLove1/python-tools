# Enhance LLM 
https://mp.weixin.qq.com/s/Hd-7J9agZ_MC-mWDaPOeHA
## ICL
in-context learning，上下文学习(ICL)是指不需要微调，只需要少数几个样例作为示例，就能在未知任务上取得不错的效果，
属于few-shot能力的一种。﻿ICL的过程，并不涉及到梯度的更新，因为整个过程不属于fine-tuning范畴。而是将一些带有标签的样本拼接起来，
作为prompt的一部分，引导模型在新的测试数据输入上生成预测结果。下面是使用ICL前后的例子。

## CoT
即：Chain-of-Thought思维链。CoT广为人知的一篇报告[4,5]：哄一哄能让GPT-3准确率暴涨61%！谷歌&东京大学研究震惊四座。
问题是：16个球中有一半是高尔夫球，这些高尔夫球中有一半是蓝色的，一共有 几个蓝色的高尔夫球？
(a) few-shot：即上述ICL方法，可以看出这种相对复杂些的问题，GPT-3解决不了。
(b) few-shot + CoT：给 few-shot例子的时候，把思维的过程也给出来，GPT-3就表现的很好。说明思维链很重要。
(c) zero-shot：不给任何例子，直接问，回答错误。
(d) zero-shot + CoT：加一句Let's think step by step，“哄一哄”就能让GPT-3很好地回答问题。

## Self-Ask
顾名思义，Self-Ask[6]即自己向自己提问。
动机：任务特定的Few-Shot Prompt需要手动想，能不能让LLMs自己设计Prompts? 我们只负责搭建通用的一些提示词且所有问题通用。
主要思路：引导LLM将一个复杂的问题拆分为简单的问题，逐个回答，然后汇总成为答案。
实施细节1：在prompt中直接提问“是否需要拆分出子问题”。
实施细节2：拆分出的子问题，不依赖LLM自己进行回答，而是调用外部搜索工具进行回答。并把结果交给LLM继续思考推理。
这个方式可以引入外部知识库来补充LLM所不知道的信息。那么如何引导LLM正确的分解问题，如何和程序交互呢？看如下演示：

## ReAct
ReAct[7]是 Reasoning 和 Acting 的缩写。
ReAct也是引导LLM将复杂的问题解决过程拆解为简单的步骤，和self-ask不同：ReAct每次让LLM输出一个当前的“ 思考 ”和“ 要做的动作 ”，这个动作并不只限于检索信息，可以是调用任何工具。LLM通过few shot的例子和工具自带的描述、 还有它自己学到的常识来决定何时调用工具以及如何调用工具。
Thought：面对这个 Question 我下一步应该做什么。
Action：执行某个动作。在 ReAct 里有三种动作，第一个是 Search[entity] 如果存在对应实体的维基页面，则返回前5句话，否则使用维基百科搜索引擎搜索前5个类似的实体；第二个是 Look[string] 它将返回包含该字符串的页面中的下一句话，类似我们使用浏览器时的 Ctrl+F 功能。第三个是 Finish[answer] 它将使用答案完成当前任务。
Observation：观察到的外部工具给到的结果，将作为新的提示输入给 ChatGPT。
上述是原作的方式，实际上还可以有其他的变体。ReAct模式在LangChain等开源工具中用的很多。
演示如下：科罗拉多造山运动东段延伸到的区域的海拔范围是多少？

## ReWOO
可以看出不管是Selk-Ask还是React，每次提问都需要把所有之前的提示词，样本，任务，历史回答等叠加起来输入下一个LLM，从而迭代的产生新推理。
一旦推理的步数变大，或者某个步骤中词元很长，就会导致下一次调用LLM时过长的输入词元，以及高度重复带来的额外计算支出。这也是ReWOO：Decoupling 
Reasoning from Observations for Effcient Augmented Language Models[8]工作的动机，Reasoning WithOut Observation将推理和观测解耦。
ReAct和ReWOO对比如下，
ReAct：当用户输入任务后，上下文提示词（Context）以及可能的样本（Exemplar）被一起传导进LLM，从而产生一个想法（T) ，行动（A)，然后调用
工具产生观察（O) 。由于LLM是无状态的（stateless）， 所有之前的提示词，样本，任务，以及T，A，O历史会被叠加起来输入下一个LLM，从而迭代的产生新推理。
ReWOO：由一个计划器（Planner）将任务分解成一个调用工具的蓝图，在获得所有工具的输出后，计划（P）和线索（E）被传导到一个解释器（Solver）进行总结并输出。
这个过程中没有对工具输出的重复词元提示，且仅需要调用两次LLM
示例如下，个人认为核心就是这个Planner，将所有要调用外部工具并获取的Evidence、需要调用LLM并获取的Evidence都提前想好，构成一个计划蓝图。剩下的工作就是调用工具填充空白的格子、利用LLM基于填充的结果做推理。

## Reflexion
Reflexion：Language Agents with Verbal Reinforcement Learning[11]。使用口头反馈来强化LLM的能力，示例如下：
