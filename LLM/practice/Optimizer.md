# Optimizer
## 1. 梯度下降法
最基本的优化方法，沿着负梯度的方向更新参数，实现如下：
```pycon
x += - learning_rate * dx
```
其中learning_rate是超参数代表学习率，被更新的变量为x，其梯度为dx，梯度->位置，很好理解。
但是梯度下降法相关的优化方法容易产生震荡，且容易被困在鞍点，迟迟不能到达全局最优值。

动量法
动量法是一类从物理中的动量获得启发的优化方法，可以简单理解为：当我们将一个小球从山上滚下来时，没有阻力的话，它的动量会越来越大，但是如果遇到了阻力，速度就会变小。实现如下：

##  动量法
```pycon
v = mu * v - learning_rate * dx # 梯度影响速度
x += v # 速度决定位置
```
变量v的初始值被定为0，超参数mu在优化过程中被视为动量，其物理意义可以视为摩擦系数，加入的这一项，
可以使得梯度方向不变的维度上速度变快，梯度方向有所改变的维度上的更新速度变慢，这样就可以加快收敛并减小震荡。
和之前不同的是梯度不会直接对位置造成影响，梯度->速度->位置。

3. RMSprop
RMSprop是一种自适应学习率方法，依旧是基于梯度对位置进行更新。为了消除梯度下降中的摆动，加入了梯度平方的指数加权平均。 
梯度大的指数加权平均就大，梯度小的指数加权平均就小，保证各维度的梯度都在一个良机，进而减少摆动。
关于指数加权平均的通俗理解可以参考
```pycon
cache = decay_rate * cache + (1 - decay_rate) * dx**2 # 梯度平方的指数加权平均
x += - learning_rate * dx / (np.sqrt(cache) + eps) # 基于梯度更新
```
其中decay_rate和eps都是超参数，每一步的变量cache的值都不同，所以可以看做自适应得对学习率进行调整。
还有一些其他效果较好的优化器，由于这些前置知识已经足够理解Adam了，所以在此不做过多介绍。

## Adam
Adam可以看做动量法和RMSprop的结合
```pycon
m = beta1*m + (1-beta1)*dx
v = beta2*v + (1-beta2)*(dx**2)
x += - learning_rate * m / (np.sqrt(v) + eps)
```
对于m和v的处理，同样使用了指数加权平均。相比于RMSprop，梯度换为了平滑的m，而cache的处理基本没有变化。
超参数beta1和beta2的初始值接近于1，因此，计算出的偏差项接近于0。

## AdamW
AdamW是在Adam+L2正则化的基础上进行改进的算法。
使用Adam优化带L2正则的损失并不有效。如果引入L2正则项，在计算梯度的时候会加上对正则项求梯度的结果。
那么如果本身比较大的一些权重对应的梯度也会比较大，由于Adam计算步骤中减去项会有除以梯度平方的累积，使得减去项偏小。
按常理说，越大的权重应该惩罚越大，但是在Adam并不是这样。而权重衰减对所有的权重都是采用相同的系数进行更新，越大的权重显然
惩罚越大。在常见的深度学习库中只提供了L2正则，并没有提供权重衰减的实现。


